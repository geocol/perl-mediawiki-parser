=head1 NAME

MediaWikiXML::PageExtractor - Extract page data from MediaWiki-dumped XML file

=head1 SYNOPSIS

  require MediaWikiXML::PageExtractor;
  
  ## Extract pages whose title matches to "HTML" and "XML" from STDIN
  ## (latest-pages-meta-current.xml) and save into the |cache/page-by-name|
  ## directory.
  MediaWikiXML::PageExtractor->save_page_xml (\*STDIN, qr/HTML|XML/);
  
  ## Print the content of "HTML" wiki page using the saved data
  ## in the |cache/page-by-name| directory.
  print MediaWikiXML::PageExtractor->get_text_from_cache ('HTML');

=head1 DESCRIPTION

The C<MediaWikiXML::PageExtractor> module can be used to extract text
source content of MediaWiki-based wiki site, such as Wikipedia, from
the dumped XML file.

=head1 SEE ALSO

Wikimedia meta-wiki - Data dumps
<http://meta.wikimedia.org/wiki/Data_dumps>.

Wikipedia (Japanese) - Database Download
<http://ja.wikipedia.org/wiki/Wikipedia:%E3%83%87%E3%83%BC%E3%82%BF%E3%83%99%E3%83%BC%E3%82%B9%E3%83%80%E3%82%A6%E3%83%B3%E3%83%AD%E3%83%BC%E3%83%89>.

=head1 AVAILABILITY

Latest version of this Perl modules is available in the git repository
<https://github.com/geocol/perl-mediawiki-parser>.

=head1 HISTORY

First version was released on 2010-09-12.

=head1 AUTHOR

Wakaba <wakaba@suikawiki.org>.

=head1 LICENSE

Copyright 2010 Wakaba <wakaba@suikawiki.org>.

Copyright 2014 Hatena <http://hatenacorp.jp/>.

This program is free software; you can redistribute it and/or modify
it under the same terms as Perl itself.

=cut
